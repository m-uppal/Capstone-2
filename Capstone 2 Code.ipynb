{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>us_viewers_in_millions</th>\n",
       "      <th>video_url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1990-03-25</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G10</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Night Out</td>\n",
       "      <td>30.3</td>\n",
       "      <td>http://www.simpsonsworld.com/video/275197507879</td>\n",
       "      <td>50816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1990-04-29</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G12</td>\n",
       "      <td>1</td>\n",
       "      <td>Krusty Gets Busted</td>\n",
       "      <td>30.4</td>\n",
       "      <td>http://www.simpsonsworld.com/video/288019523914</td>\n",
       "      <td>62561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1990-10-11</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F03</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart Gets an \"F\"</td>\n",
       "      <td>33.6</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260539459671</td>\n",
       "      <td>59575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1990-11-01</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F01</td>\n",
       "      <td>2</td>\n",
       "      <td>Two Cars in Every Garage and Three Eyes on Eve...</td>\n",
       "      <td>26.1</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260537411822</td>\n",
       "      <td>64959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-11-15</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F08</td>\n",
       "      <td>2</td>\n",
       "      <td>Dead Putting Society</td>\n",
       "      <td>25.4</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260539459670</td>\n",
       "      <td>50691.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          image_url  imdb_rating  \\\n",
       "0  10  http://static-media.fxx.com/img/FX_Networks_-_...          7.4   \n",
       "1  12  http://static-media.fxx.com/img/FX_Networks_-_...          8.3   \n",
       "2  14  http://static-media.fxx.com/img/FX_Networks_-_...          8.2   \n",
       "3  17  http://static-media.fxx.com/img/FX_Networks_-_...          8.1   \n",
       "4  19  http://static-media.fxx.com/img/FX_Networks_-_...          8.0   \n",
       "\n",
       "   imdb_votes  number_in_season  number_in_series original_air_date  \\\n",
       "0      1511.0                10                10        1990-03-25   \n",
       "1      1716.0                12                12        1990-04-29   \n",
       "2      1638.0                 1                14        1990-10-11   \n",
       "3      1457.0                 4                17        1990-11-01   \n",
       "4      1366.0                 6                19        1990-11-15   \n",
       "\n",
       "   original_air_year production_code  season  \\\n",
       "0               1990            7G10       1   \n",
       "1               1990            7G12       1   \n",
       "2               1990            7F03       2   \n",
       "3               1990            7F01       2   \n",
       "4               1990            7F08       2   \n",
       "\n",
       "                                               title  us_viewers_in_millions  \\\n",
       "0                                  Homer's Night Out                    30.3   \n",
       "1                                 Krusty Gets Busted                    30.4   \n",
       "2                                   Bart Gets an \"F\"                    33.6   \n",
       "3  Two Cars in Every Garage and Three Eyes on Eve...                    26.1   \n",
       "4                               Dead Putting Society                    25.4   \n",
       "\n",
       "                                         video_url    views  \n",
       "0  http://www.simpsonsworld.com/video/275197507879  50816.0  \n",
       "1  http://www.simpsonsworld.com/video/288019523914  62561.0  \n",
       "2  http://www.simpsonsworld.com/video/260539459671  59575.0  \n",
       "3  http://www.simpsonsworld.com/video/260537411822  64959.0  \n",
       "4  http://www.simpsonsworld.com/video/260539459670  50691.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe out of csv files: simpsons_episodes.csv (contains episode ratings) \n",
    "# & simpsons_script_lines.csv (constains episode scripts)\n",
    "df_1 = pd.read_csv('raw data source/simpsons_episodes.csv')\n",
    "\n",
    "#inspect dataframe using .head()\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8084: expected 13 fields, saw 20\\nSkipping line 52607: expected 13 fields, saw 21\\nSkipping line 59910: expected 13 fields, saw 21\\n'\n",
      "b'Skipping line 71801: expected 13 fields, saw 20\\nSkipping line 73539: expected 13 fields, saw 21\\nSkipping line 77230: expected 13 fields, saw 21\\nSkipping line 78953: expected 13 fields, saw 21\\nSkipping line 81138: expected 13 fields, saw 20\\nSkipping line 86746: expected 13 fields, saw 22\\nSkipping line 101154: expected 13 fields, saw 21\\nSkipping line 115438: expected 13 fields, saw 20\\nSkipping line 117573: expected 13 fields, saw 22\\nSkipping line 130610: expected 13 fields, saw 22\\n'\n",
      "b'Skipping line 152970: expected 13 fields, saw 22\\nSkipping line 153017: expected 13 fields, saw 20\\nSkipping line 153018: expected 13 fields, saw 30\\nSkipping line 154080: expected 13 fields, saw 20\\nSkipping line 154082: expected 13 fields, saw 20\\nSkipping line 154084: expected 13 fields, saw 20\\nSkipping line 154086: expected 13 fields, saw 20\\nSkipping line 154089: expected 13 fields, saw 23\\nSkipping line 154165: expected 13 fields, saw 21\\nSkipping line 156872: expected 13 fields, saw 20\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>True</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          True          464          3.0              Miss Hoover   \n",
       "1          True            9          3.0             Lisa Simpson   \n",
       "2          True          464          3.0              Miss Hoover   \n",
       "3          True            9          3.0             Lisa Simpson   \n",
       "4          True           40          3.0  Edna Krabappel-Flanders   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text  word_count  \n",
       "0  no actually it was a little of both sometimes ...        31.0  \n",
       "1                                wheres mr bergstrom         3.0  \n",
       "2  i dont know although id sure like to talk to h...        22.0  \n",
       "3                          that life is worth living         5.0  \n",
       "4  the polls will be open from now until the end ...        33.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note parse errors ; tried 'sep',  error_bad_lines=False, \n",
    "df_2 = pd.read_csv('raw data source/simpsons_script_lines.csv',  error_bad_lines=False)\n",
    "\n",
    "#inspect dataframe using .head()\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>...</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>...</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>...</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>8000</td>\n",
       "      <td>true</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>...</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>10000</td>\n",
       "      <td>true</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>theres no time to be careful</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>...</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>10000</td>\n",
       "      <td>true</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>were late</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  imdb_rating  imdb_votes  \\\n",
       "1  http://static-media.fxx.com/img/FX_Networks_-_...          8.2      3734.0   \n",
       "1  http://static-media.fxx.com/img/FX_Networks_-_...          8.2      3734.0   \n",
       "1  http://static-media.fxx.com/img/FX_Networks_-_...          8.2      3734.0   \n",
       "1  http://static-media.fxx.com/img/FX_Networks_-_...          8.2      3734.0   \n",
       "1  http://static-media.fxx.com/img/FX_Networks_-_...          8.2      3734.0   \n",
       "\n",
       "   number_in_season  number_in_series original_air_date  original_air_year  \\\n",
       "1                 1                 1        1989-12-17               1989   \n",
       "1                 1                 1        1989-12-17               1989   \n",
       "1                 1                 1        1989-12-17               1989   \n",
       "1                 1                 1        1989-12-17               1989   \n",
       "1                 1                 1        1989-12-17               1989   \n",
       "\n",
       "  production_code  season                              title  ...  \\\n",
       "1            7G08       1  Simpsons Roasting on an Open Fire  ...   \n",
       "1            7G08       1  Simpsons Roasting on an Open Fire  ...   \n",
       "1            7G08       1  Simpsons Roasting on an Open Fire  ...   \n",
       "1            7G08       1  Simpsons Roasting on an Open Fire  ...   \n",
       "1            7G08       1  Simpsons Roasting on an Open Fire  ...   \n",
       "\n",
       "                                        raw_text timestamp_in_ms  \\\n",
       "1   (Street: ext. street - establishing - night)            8000   \n",
       "1                        (Car: int. car - night)            8000   \n",
       "1            Marge Simpson: Ooo, careful, Homer.            8000   \n",
       "1  Homer Simpson: There's no time to be careful.           10000   \n",
       "1                     Homer Simpson: We're late.           10000   \n",
       "\n",
       "   speaking_line  character_id  location_id raw_character_text  \\\n",
       "1          false           NaN          1.0                NaN   \n",
       "1          false           NaN          2.0                NaN   \n",
       "1           true             1          2.0      Marge Simpson   \n",
       "1           true             2          2.0      Homer Simpson   \n",
       "1           true             2          2.0      Homer Simpson   \n",
       "\n",
       "  raw_location_text                    spoken_words  \\\n",
       "1            Street                             NaN   \n",
       "1               Car                             NaN   \n",
       "1               Car            Ooo, careful, Homer.   \n",
       "1               Car  There's no time to be careful.   \n",
       "1               Car                     We're late.   \n",
       "\n",
       "                normalized_text  word_count  \n",
       "1                           NaN         NaN  \n",
       "1                           NaN         NaN  \n",
       "1             ooo careful homer         3.0  \n",
       "1  theres no time to be careful         6.0  \n",
       "1                     were late         2.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join df_1 and df_2, \n",
    "# from df_1 join id on episode_id in df_2\n",
    "df_3 = df_1.set_index('id').join(df_2.set_index('episode_id'))\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_url', 'imdb_rating', 'imdb_votes', 'number_in_season',\n",
       "       'number_in_series', 'original_air_date', 'original_air_year',\n",
       "       'production_code', 'season', 'title', 'us_viewers_in_millions',\n",
       "       'video_url', 'views', 'id', 'number', 'raw_text', 'timestamp_in_ms',\n",
       "       'speaking_line', 'character_id', 'location_id', 'raw_character_text',\n",
       "       'raw_location_text', 'spoken_words', 'normalized_text', 'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imdb_rating                                       raw_text\n",
       "1          8.2   (Street: ext. street - establishing - night)\n",
       "1          8.2                        (Car: int. car - night)\n",
       "1          8.2            Marge Simpson: Ooo, careful, Homer.\n",
       "1          8.2  Homer Simpson: There's no time to be careful.\n",
       "1          8.2                     Homer Simpson: We're late."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns from df_3 that are not: 'imdb_rating',  'raw_text', 'episode_id'/'id'\n",
    "\n",
    "df_4 = df_3.drop(['image_url', \n",
    "                  'imdb_votes', \n",
    "                  'number_in_season',\n",
    "                  'number_in_series', \n",
    "                  'original_air_date', \n",
    "                  'original_air_year',\n",
    "                  'production_code', \n",
    "                  'season', \n",
    "                  'title', \n",
    "                  'us_viewers_in_millions',\n",
    "                  'video_url', \n",
    "                  'views', \n",
    "                  'id', \n",
    "                  'number', \n",
    "                  'timestamp_in_ms',\n",
    "                  'speaking_line', \n",
    "                  'character_id', \n",
    "                  'location_id', \n",
    "                  'raw_character_text',\n",
    "                  'raw_location_text', \n",
    "                  'spoken_words', \n",
    "                  'normalized_text', \n",
    "                  'word_count'],\n",
    "                   axis=1)\n",
    "\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mandi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = df_4[df_4['raw_text'].notna()]['raw_text'].array\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# training data \n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 5.4590022e-03  9.2690423e-02 -1.3117660e-02 -6.8165652e-02\n",
      " -6.6192895e-02  1.7408142e-03 -3.4354977e-02 -5.7723835e-02\n",
      "  1.8227996e-02  9.9459663e-05 -2.6386639e-02 -3.1416308e-02\n",
      " -1.8038391e-04 -1.0770970e-01 -2.2901991e-02 -6.4087678e-03\n",
      "  6.6136450e-02 -1.1120888e-01  9.1394193e-02 -7.6076597e-02]\n",
      "[('5794', 0.9794455766677856), ('6088', 0.9780064821243286), ('2161', 0.9757506847381592), ('2165', 0.9748499989509583), ('99667', 0.9743762016296387), ('24675', 0.9733880758285522), ('38136', 0.9730510711669922), ('6131', 0.9708636403083801), ('31209', 0.9701455235481262), ('8189', 0.9672805070877075)]\n",
      "[ 2.6026225   5.699102   -0.23596713 -1.0734864   2.5484433   0.28699082\n",
      "  2.5214052   0.7284073  -0.15339501 -0.843765    0.63446325  0.3911802\n",
      "  0.06404012 -0.86428076 -3.1703653  -0.52673745  0.83596957 -0.5162934\n",
      "  1.176333   -0.53198934]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(data[0].lower())\n",
    "v1 = model.infer_vector(test_data) # creates vector per row\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list = [] #\n",
    "for episode_text in data: \n",
    "    test_data = word_tokenize(episode_text.lower())\n",
    "    v1 = model.infer_vector(test_data) # creates vector per row\n",
    "    vector_list.append(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np  \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "forest = RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "#  Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Train classifer\n",
    "forest.fit(X_train, y_train) \n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = forest.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train set: 1.0\n",
      "Accuracy test set: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "print(\"Accuracy for train set:\", forest.score(X_train, y_train)) \n",
    "print(\"Accuracy test set:\",forest.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94  4]\n",
      " [ 4 98]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        98\n",
      "           1       0.96      0.96      0.96       102\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report \n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the train set: 0.95125\n",
      "Accuracy score on the test set:  0.95\n",
      "Recall, test set: 0.95\n",
      "Precision, test set: 0.95\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Classifier\n",
    "reg = LogisticRegression() \n",
    "\n",
    "# train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42) \n",
    "\n",
    "# fit\n",
    "reg.fit(X_train, y_train) \n",
    "\n",
    "# predict\n",
    "y_pred = reg.predict(X_test)  \n",
    "\n",
    "# metrics/evaluate model\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "print(\"Accuracy score on the train set:\", reg.score(X_train, y_train)) \n",
    "print(\"Accuracy score on the test set: \", reg.score(X_test, y_test))\n",
    "print('Recall, test set: %0.2f' % recall_score(y_test, y_pred))\n",
    "print('Precision, test set: %0.2f' % precision_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
